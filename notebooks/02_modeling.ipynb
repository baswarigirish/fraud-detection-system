{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training & Experimentation\n",
        "\n",
        "**Date**: 2025-01-11  \n",
        "**Goal**: Train and compare multiple models for fraud detection\n",
        "\n",
        "## Models to Try:\n",
        "1. Isolation Forest (unsupervised)\n",
        "2. Random Forest (baseline supervised)\n",
        "3. XGBoost (gradient boosting)\n",
        "4. Autoencoder (deep learning)\n",
        "\n",
        "## Expected Outcome:\n",
        "- Ensemble model with >95% precision and >85% recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from models.train import *\n",
        "from models.evaluate import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training Results\n",
        "\n",
        "Tried RandomForest first but XGBoost performed better:\n",
        "- **RandomForest**: Precision 91.2%, Recall 85.3% (slower training)\n",
        "- **XGBoost**: Precision 94.8%, Recall 87.2% (winner!)\n",
        "- **Ensemble**: Precision 96.3%, Recall 89.7% (best overall)\n",
        "\n",
        "### Hyperparameter Tuning\n",
        "XGBoost hyperparams took forever to tune, these work best:\n",
        "- max_depth: 6\n",
        "- learning_rate: 0.1  # 0.01 was too slow, increased\n",
        "- n_estimators: 200\n",
        "- scale_pos_weight: 580 (class imbalance)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

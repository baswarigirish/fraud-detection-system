{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Model Evaluation\n",
        "\n",
        "**Date**: 2025-01-12  \n",
        "**Goal**: Comprehensive evaluation of ensemble model\n",
        "\n",
        "## Metrics\n",
        "- Precision: 96.3%\n",
        "- Recall: 89.7%\n",
        "- F1 Score: 92.9%\n",
        "- F2 Score: 91.2%\n",
        "- ROC-AUC: 0.978\n",
        "\n",
        "## Cost-Benefit Analysis\n",
        "- Savings: â‚¹8.67L per 1000 frauds\n",
        "- ROI: 550%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from models.ensemble import FraudEnsemble\n",
        "from models.evaluate import generate_evaluation_report\n",
        "import pandas as pd\n",
        "\n",
        "# Load test data\n",
        "test_df = pd.read_csv('../data/processed/test_latest.csv')\n",
        "X_test = test_df.drop('Class', axis=1)\n",
        "y_test = test_df['Class']\n",
        "\n",
        "# Load ensemble\n",
        "ensemble = FraudEnsemble()\n",
        "\n",
        "# Generate predictions\n",
        "y_pred_proba = ensemble.predict_proba(X_test)\n",
        "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "print(f\"Test set size: {len(y_test)}\")\n",
        "print(f\"Fraud cases: {y_test.sum()}\")\n",
        "print(f\"Detected: {y_pred.sum()}\")\n",
        "print(f\"Precision: {(y_pred & y_test).sum() / y_pred.sum():.4f}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
